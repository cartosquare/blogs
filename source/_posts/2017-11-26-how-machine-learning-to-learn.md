---
title: 机器如何学习？
date: 2017-11-26 20:13:39
tags:
  - 深度学习
  - 机器学习
  - 随机梯度下降
  - 求导
  - 链式法则
  - 自动微分
---

机器学习的目标是获得一组最优的参数，这些参数决定了评分函数，因此我们要找一个最优的评分函数。实际上通过引入损失函数，我们可以通过最小化损失函数来不断更新评分函数的参数，具体来说，机器学习可以通过一种迭代的技术，使得每一步的迭代之后，损失函数的输出都能减小，因此在有限步迭代之后，损失函数就能达到最小值点（当然这是一个局部最小值）。在每一次迭代之后，我们都会根据损失函数的输出更新评分函数的参数，这样就达到了学习的目的。

<!-- more -->
## 随机梯度下降
随机梯度下降（Stochastic gradient decent，SGD）就是这样一种迭代技术。基于函数f的梯度负方向$-\Delta f$是这个函数下降最快的方向，SGD通过从某个点$w_0$出发，不断沿着梯度负方向前进一个小步长$\eta$来达到求函数f的局部最优解的目的：
$$w^*=argmin_wf(w) \qquad (1)$$

假设经过I步达到最优点$w^*$，那么寻找最优解的过程如下：

$$
w\_1 = w\_0 - \eta \Delta f(w\_0) \\\\
w\_2 = w\_1 - \eta \Delta f(w\_1) \\\\
\cdot\cdot\cdot \\\\
w^* = w\_{I-1} - \eta \Delta f(w\_{I-1}) 
$$

这里步长$\eta$的设置一般是动态的，如果设定为固定值，从点$w_0$到最优点的轨迹有点像Z字形，收敛的时间会很长，所以一般都会动态地减小步长值。

### 导数
为了进行迭代，我们必须要在每一步里计算当前点的梯度，也就是导数：
$$\frac{d}{dx}f = \lim\_{x\to x\_0}\frac {f(x) - f(x\_0)}{x - x\_0} $$

考虑到实际上机器学习的函数十分复杂，首先它是评分函数和损失函数组成的复合函数：
$$F = Loss(Score(x))$$
其次，评分函数有可能十分复杂，比如，在深度学习里，评分函数是一系列网络层的复合，假设有N个网络层，每个网络的函数是$K_{i}$，那么评分函数就是这N个网络层的复合函数：

$$Score(x)=K\_{n}(K\_{n-1}(...K\_1(x)))$$

### 链式法则
因此一般的机器学习框架都会采用一种后向自动微分的方式求导数。这种方式不仅不需要使用微积分的分析方式求出导数的表达式，运算效率还特别高。其中的原理是运用了复合函数求导的链式法则：
$$\frac {d}{dx}f(y(x)) = \frac {df}{dy} \frac{dy}{dx}$$

比如，要计算
$$y = sin(x^2)$$
在$x=1$处的导数，首先我们分成几步计算y的值：
$$
\\begin{align}
& x\_0 = 1 \\\\
& x\_1 = x\_0^2 = 1 \\\\
& x\_2 = sin(x\_1) = 0.8415 \\\\
& y = x\_2 = 0.8415
\\end{align}
$$
计算导数时，只需反过来对每一步求导即可：

$$
\\begin{align}
& \bar{v\_2} = \bar y = 1 \\\\ 
& \bar{v\_1} = \bar{v\_2}\frac{dv\_2}{dv\_1} = \bar{v\_2} \times cos(x\_1) = 0.8415 \\\\
& \bar{v\_0} = \bar{v\_1}\frac{dv\_1}{dv\_0} = \bar{v\_1} \times 2x\_0 = 1.0806 
\\end{align}
$$

对于这个例子，我们可以很快验证，函数f的导数是：
$$
\frac{dy}{dx} = cos(x^2)\cdot 2x
$$
代入x=1，得：
$$
cos(1)\times 2 = 1.0806
$$

因此，自动微分的好处在于我们不必显示地用微积分的方法求解导数表达式。在caffe2中，我们只需要对每一层的函数编码导数的表达式，整个神经网络表示的大函数的梯度则通过自动微分的方式汇集每一层的导数值得来。

### SVM Loss的梯度
## SoftMax Loss的梯度

